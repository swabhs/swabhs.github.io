<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Swabha Swayamdipta </title> <meta name="author" content="Swabha Swayamdipta"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%81&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://swabhs.github.io/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/group/">group </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Swabha</span> Swayamdipta </h1> <p class="desc"><a href="https://wise.usc.edu/programs/grants-and-awards/faculty/wise-gabilan-assistant-professorship/" rel="external nofollow noopener" target="_blank">Gabilan</a> Assistant Professor ‚Ä¢ <a href="https://cs.usc.edu/" rel="external nofollow noopener" target="_blank">USC Viterbi CS</a> ‚Ä¢ Associate Director of USC <a href="https://cais.usc.edu/" rel="external nofollow noopener" target="_blank">Center for AI and Society</a> ‚Ä¢ Amazon Scholar ‚Ä¢ <a href="https://nlp.usc.edu" rel="external nofollow noopener" target="_blank">USC NLP</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile-pic-trojan-blurred-v1-480.webp 480w,/assets/img/profile-pic-trojan-blurred-v1-800.webp 800w,/assets/img/profile-pic-trojan-blurred-v1-1400.webp 1400w," sizes="(min-width: 1200px) 351.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/profile-pic-trojan-blurred-v1.png?d0b73bafb8905f2884a6d44144badccd" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="profile-pic-trojan-blurred-v1.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>My goal is to design frameworks that allow robust, and reliable frameworks that allow AI systems to be broadly and safely deployed, especially in applications with societal implications. Three directions that this corresponds to are:</p> <ul> <li> <dl> <dt>Safety-Critical, Robust and Reliable Language Models:</dt> <dd>What cannot be measured, cannot be improved. How can we reliably compare the generative capabilities of language models, and ensure our assessment is robust? How can we tell if performance match can translate to application safety, especially when there are societal implications? How can we evaluate new capabilities in LLMs when we do not necessarily know the correct answer?</dd> </dl> </li> <li> <dl> <dt>Understanding the Mechanisms that Drive Language Models:</dt> <dd>Even the most reliable evaluation may not reveal much about the mechanisms driving powerful yet opaque models. What do model geometries reveal about the processes underlying our models, and how can we improve models through different designs? Are models by design limited to making some choices which can uniquely identify them?</dd> </dl> </li> <li> <dl> <dt>Private and Safe Human-AI Collaboration for High-Stakes Applications:</dt> <dd>AI technologies are designed by humans and for humans, the future of AI involves cooperation and collaboration with humans. How can we say when a general-purpose model will reliably serve the custom utility for a human user? Where can these technologies complement human capabilities and where not?</dd> </dl> </li> </ul> <p>These challenges require novel and creative techniques for redesigning generative evaluation to keep pace with model performance. This brings together a broad array of empirical research with theoretical fundamentals underlying language models.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%;color:black">Jan 21, 2026</th> <td style="color:black"> Had my first <a href="https://www.youtube.com/watch?v=rSC7L5WikcE" rel="external nofollow noopener" target="_blank">podcast appearance</a> for the <a href="https://www.youtube.com/@WomeninAIResearch" rel="external nofollow noopener" target="_blank">Women in AI Research podcast</a>, hosted by Jekaterina Novikova. </td> </tr> <tr> <th scope="row" style="width: 20%;color:black">Dec 08, 2025</th> <td style="color:black"> Honored to receive an <a href="https://www.aim-ahead.net/programs/aim-ahead-hub-specific-projects/" rel="external nofollow noopener" target="_blank">NIH AIM-Ahead Hub-Specific Project Award</a> with <a href="https://ssw.unc.edu/employees/eric-rice/" rel="external nofollow noopener" target="_blank">Eric Rice</a> on using AI for helping Youth Homelessness Serving Organizations. </td> </tr> <tr> <th scope="row" style="width: 20%;color:black">Apr 25, 2025</th> <td style="color:black"> Honored to receive a sponsored research grant by Apple. </td> </tr> <tr> <th scope="row" style="width: 20%;color:black">Apr 23, 2025</th> <td style="color:black"> DILL lab has newly minted entrepreneurs: Jaspreet Ranjit and Aryan Gulati are the <a href="https://viterbischool.usc.edu/news/2025/05/caseflo-wins-the-2025-min-family-challenge/" rel="external nofollow noopener" target="_blank">Min Family Challenge</a> winners in 2025. </td> </tr> <tr> <th scope="row" style="width: 20%;color:black">Apr 23, 2025</th> <td style="color:black"> DILL Lab wins two awards at <a href="https://sites.google.com/usc.edu/showcais-2025/awards?authuser=1" rel="external nofollow noopener" target="_blank">ShowCAIS 2025</a>: best poster by undergrad Risha Surana and runner-up best oral presentation by Jaspreet Ranjit. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <span>See <a href="../publications/">here</a> for a full list.</span> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="finlayson2025languagemodelforgeryresistantsignature" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2510.14086" target="_blank" rel="external nofollow noopener"><b>Every Language Model Has a Forgery-Resistant Signature</b></a></div> <div class="author"> Matthew Finlayson ,¬†Xiang Ren ,¬†and¬†<span style="font-weight:bold">Swabha Swayamdipta</span> </div> <div class="periodical"> <em>Proc. of ICLR</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> </div> <div class="abstract hidden"> <p>The ubiquity of closed-weight language models with public-facing APIs has generated interest in forensic methods, both for extracting hidden model details (e.g., parameters) and for identifying models by their outputs. One successful approach to these goals has been to exploit the geometric constraints imposed by the language model architecture and parameters. In this work, we show that a lesser-known geometric constraint‚Äìnamely, that language model outputs lie on the surface of a high-dimensional ellipse‚Äìfunctions as a signature for the model and can be used to identify the source model of a given output. This ellipse signature has unique properties that distinguish it from existing model-output association methods like language model fingerprints. In particular, the signature is hard to forge: without direct access to model parameters, it is practically infeasible to produce log-probabilities (logprobs) on the ellipse. Secondly, the signature is naturally occurring, since all language models have these elliptical constraints. Thirdly, the signature is self-contained, in that it is detectable without access to the model inputs or the full weights. Finally, the signature is compact and redundant, as it is independently detectable in each logprob output from the model. We evaluate a novel technique for extracting the ellipse from small models and discuss the practical hurdles that make it infeasible for production-scale models. Finally, we use ellipse signatures to propose a protocol for language model output verification, analogous to cryptographic symmetric-key message authentication systems.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="yauney2025reliablelanguagemodelmicrobenchmarking" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2510.08730" target="_blank" rel="external nofollow noopener"><b>How Reliable is Language Model Micro-Benchmarking?</b></a></div> <div class="author"> Gregory Yauney ,¬†Shahzaib Saqib Warraich ,¬†and¬†<span style="font-weight:bold">Swabha Swayamdipta</span> </div> <div class="periodical"> <em>Proc. of ICLR</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/dill-lab/micro-benchmarking-reliability" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Micro-benchmarking offers a solution to the often prohibitive time and cost of language model development: evaluate on a very small subset of existing benchmarks. Can these micro-benchmarks, however, rank models as consistently as the full benchmarks they replace? And can they rank models more consistently than selecting a random subset of data points? In many scenarios, we find that the answer is no. We introduce a meta-evaluation measure for micro-benchmarking which investigates how well a micro-benchmark can rank two models as a function of their performance difference on the full benchmark. This approach can determine which model pairs can be ranked correctly by a micro-benchmark, allowing for a finer-grained analysis of the trade-off between micro-benchmark size and reliability. Prior work has suggested selecting as few as 10 examples; we find that no micro-benchmarking method can consistently rank model pairs 3.5 points of accuracy apart on MMLU-Pro or 4 points apart on BIG-bench Hard. In order to consistently rank model pairs with relatively similar performances, we show that often as many as 250 examples must be selected, at which point random sampling is competitive with existing micro-benchmarking methods. When comparing only 8B instruction-tuned models on MMLU-Pro micro-benchmarks with 25 examples, we find that more than half of pairwise comparisons are not likely to be preserved. Our work provides actionable guidance for both micro-benchmark users and developers in navigating the trade-off between evaluation efficiency and reliability.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="nazir2025betterlanguagemodelinversion" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2506.17090" target="_blank" rel="external nofollow noopener"><b>Better Language Model Inversion by Compactly Representing Next-Token Distributions</b></a></div> <div class="author"> Murtaza Nazir ,¬†Matthew Finlayson ,¬†John X. Morris ,¬†Xiang Ren ,¬†and¬†<span style="font-weight:bold">Swabha Swayamdipta</span> </div> <div class="periodical"> <em>Proc. of NeurIPS</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="" class="abstract badge" role="button"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model‚Äôs system message. We propose a new method ‚Äì prompt inversion from logprob sequences (PILS) ‚Äì that recovers hidden prompts by gleaning clues from the model‚Äôs next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2‚Äì3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5‚Äì27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> </div> <div id="ranjit2025nvdrs" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2508.18541" target="_blank" rel="external nofollow noopener"><b>Uncovering Intervention Opportunities for Suicide Prevention with Language Model Assistants</b></a></div> <div class="author"> Jaspreet Ranjit ,¬†Hyundong J. Cho ,¬†Claire J. Smerdon ,¬†Yoonsoo Nam ,¬†Myles Phung ,¬†Jonathan May ,¬†John R. Blosnich ,¬†and¬†<span style="font-weight:bold">Swabha Swayamdipta</span> </div> <div class="periodical"> <em>EAAMO / NeurIPS Workshop on GenAI for Health</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://dill-lab.github.io/interventions_lm_assistants/" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>The National Violent Death Reporting System (NVDRS) documents information about suicides in the United States, including free text narratives (e.g., circumstances surrounding a suicide). In a demanding public health data pipeline, annotators manually extract structured information from death investigation records following extensive guidelines developed painstakingly by experts. In this work, we facilitate data-driven insights from the NVDRS data to support the development of novel suicide interventions by investigating the value of language models (LMs) as efficient assistants to these (a) data annotators and (b) experts. We find that LM predictions match existing data annotations about 85% of the time across 50 NVDRS variables. In the cases where the LM disagrees with existing annotations, expert review reveals that LM assistants can surface annotation discrepancies 38% of the time. Finally, we introduce a human-in-the-loop algorithm to assist experts in efficiently building and refining guidelines for annotating new variables by allowing them to focus only on providing feedback for incorrect LM predictions. We apply our algorithm to a real-world case study for a new variable that characterizes victim interactions with lawyers and demonstrate that it achieves comparable annotation quality with a laborious manual approach. Our findings provide evidence that LMs can serve as effective assistants to public health researchers who handle sensitive data in high-stakes scenarios.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CoLM</abbr> </div> <div id="wang2025teachingmodelsunderstandbut" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2505.03052" target="_blank" rel="external nofollow noopener"><b>Teaching Models to Understand (but not Generate) High-risk Data</b></a></div> <div class="author"> Ryan Wang ,¬†Matthew Finlayson ,¬†Luca Soldaini ,¬†<span style="font-weight:bold">Swabha Swayamdipta</span>,¬†and¬†Robin Jia </div> <div class="periodical"> <em>Conference on Language Modeling</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/ryanyxw/llm-decouple" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Language model developers typically filter out high-risk content ‚Äì such as toxic or copyrighted text ‚Äì from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models‚Äô ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model‚Äôs context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models‚Äô understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fictitious-watermarks-480.webp 480w,/assets/img/publication_preview/fictitious-watermarks-800.webp 800w,/assets/img/publication_preview/fictitious-watermarks-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/fictitious-watermarks.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fictitious-watermarks.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="cui2025robustdatawatermarkinglanguage" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2503.04036" target="_blank" rel="external nofollow noopener"><b>Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge</b></a></div> <div class="author"> Xinyue Cui ,¬†Johnny Tian-Zheng Wei ,¬†<span style="font-weight:bold">Swabha Swayamdipta</span>,¬†and¬†Robin Jia </div> <div class="periodical"> <em>Findings of ACL</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/dill-lab/Fictitious_Fact_Watermarks" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques primarily focus on effective memorization after pretraining, while overlooking challenges that arise in other stages of the LLM pipeline, such as the risk of watermark filtering during data preprocessing, or potential forgetting through post-training, or verification difficulties due to API-only access. We propose a novel data watermarking approach that injects coherent and plausible yet fictitious knowledge into training data using generated passages describing a fictitious entity and its associated attributes. Our watermarks are designed to be memorized by the LLM through seamlessly integrating in its training data, making them harder to detect lexically during preprocessing. We demonstrate that our watermarks can be effectively memorized by LLMs, and that increasing our watermarks‚Äô density, length, and diversity of attributes strengthens their memorization. We further show that our watermarks remain robust throughout LLM development, maintaining their effectiveness after continual pretraining and supervised finetuning. Finally, we show that our data watermarks can be evaluated even under API-only access via question answering.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/eliwhy-480.webp 480w,/assets/img/publication_preview/eliwhy-800.webp 800w,/assets/img/publication_preview/eliwhy-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/eliwhy.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="eliwhy.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="joshi2025eliwhy" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2506.14200" target="_blank" rel="external nofollow noopener"><b>ELI-Why: Evaluating the Pedagogical Utility of LLM Explanations</b></a></div> <div class="author"> Brihi Joshi ,¬†Keyu He ,¬†Sahana Ramnath ,¬†Sadra Sabouri ,¬†Kaitlyn Zhou ,¬†Souti Chattopadhyay ,¬†<span style="font-weight:bold">Swabha Swayamdipta</span>,¬†and¬†Xiang Ren </div> <div class="periodical"> <em>Findings of ACL</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/INK-USC/ELI-Why" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an "educator" to assess model explanations‚Äô fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/separability-480.webp 480w,/assets/img/publication_preview/separability-800.webp 800w,/assets/img/publication_preview/separability-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/separability.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="separability.jpeg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ghosh2024compare" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2407.01878" target="_blank" rel="external nofollow noopener"><b>Compare without Despair: Reliable Preference Evaluation with Generation Separability</b></a></div> <div class="author"> Sayan Ghosh ,¬†Tejas Srinivasan ,¬†and¬†<span style="font-weight:bold">Swabha Swayamdipta</span> </div> <div class="periodical"> <em>In Findings of EMNLP</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/dill-lab/separability" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Human evaluation of generated language through pairwise preference judgments is pervasive. However, under common scenarios, such as when generations from a model pair are very similar, or when stochastic decoding results in large variations in generations, it results in inconsistent preference ratings. We address these challenges by introducing a meta-evaluation measure, separability, which estimates how suitable a test instance is for pairwise preference evaluation. For a candidate test instance, separability samples multiple generations from a pair of models, and measures how distinguishable the two sets of generations are. Our experiments show that instances with high separability values yield more consistent preference ratings from both human- and auto-raters. Further, the distribution of separability allows insights into which test benchmarks are more valuable for comparing models. Finally, we incorporate separability into ELO ratings, accounting for how suitable each test instance might be for reliably ranking LLMs. Overall, separability has implications for consistent, efficient and robust preference evaluation of LLMs with both human- and auto-raters.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/oath_frames-480.webp 480w,/assets/img/publication_preview/oath_frames-800.webp 800w,/assets/img/publication_preview/oath_frames-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/oath_frames.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="oath_frames.jpeg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ranjit2024oath" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2406.14883" target="_blank" rel="external nofollow noopener"><b>OATH-Frames: Characterizing Online Attitudes Towards Homelessness via LLM Assistants</b></a></div> <div class="author"> Jaspreet Ranjit ,¬†Brihi Joshi ,¬†Rebecca Dorn ,¬†Laura Petry ,¬†Olga Koumoundouros ,¬†Jayne Bottarini ,¬†Peichen Liu ,¬†Eric Rice ,¬†and¬†<span style="font-weight:bold">Swabha Swayamdipta</span> </div> <div class="periodical"> <em>In Proceedings of EMNLP</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://dill-lab.github.io/oath-frames/" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Blog</span></a> <a href="https://github.com/dill-lab/oath-frames" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> <a class="abstract badge" role="button"><span style="color:var(--global-theme-color);font-weight:bold">Outstanding Paper Award @ EMNLP‚Äô24; Best Poster Award @ ShowCAIS‚Äô24</span></a> </div> <div class="abstract hidden"> <p>Homelessness in the U.S. is widespread; individual beliefs and attitudes towards homelessness‚Äîoften expressed on social media are complex and nuanced (e.g. critical as well as sympathetic). Such attitudes can be challenging to summarize at scale, obfuscating the broader public opinion which advocacy organizations use to guide public policy and reform efforts. Our work proposes an approach to enable a large-scale study on homelessness via two major contributions. First, with the help of domain experts in social work and their trainees, we characterize Online Attitudes towards Homelessness in nine hierarchical frames (OATH-Frames) on a collection of 4K social media posts. Further, in an effort to ease the annotation of these frames, we employ GPT-4 as an LLM assistant to the experts; GPT-4 + Expert annotation presents an attractive trade off owing to a 6.5√ó speedup in annotation time despite only incurring a 2 point F1 difference in annotation performance. Our effort results in a collection of 8K social media posts labeled by domain and trained experts (with and without GPT-4 assistance). Second, using predicted OATH-Frames on a Flan-T5-Large model trained on our data, we perform a large-scale analysis on 2.4M posts on homelessness. We find that posts that contain mentions of west coast states express more harmful generalizations of people experiencing homelessness (PEH) compared to posts about east coast states. We also find marked differences in attitudes across vulnerable populations as they are compared to PEH as being either more or less deserving of aid.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">COLM</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/logits-480.webp 480w,/assets/img/publication_preview/logits-800.webp 800w,/assets/img/publication_preview/logits-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/logits.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="logits.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="finlayson2024logits" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2403.09539" target="_blank" rel="external nofollow noopener"><b>Logits of API-Protected LLMs Leak Proprietary Information</b></a></div> <div class="author"> Matthew Finlayson ,¬†Xiang Ren ,¬†and¬†<span style="font-weight:bold">Swabha Swayamdipta</span> </div> <div class="periodical"> <em>In Proceedings of COLM</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> </div> <div class="abstract hidden"> <p>The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI‚Äôs gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM‚Äôs hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and even estimating the output layer parameters. Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI‚Äôs gpt-3.5-turbo to be about 4,096. Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="ethayarajh2021informationtheoretic" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2110.08420" target="_blank" rel="external nofollow noopener"><b>Understanding Dataset Difficulty with ùí±-Usable Information</b></a></div> <div class="author"> Kawin Ethayarajh ,¬†Yejin Choi ,¬†and¬†<span style="font-weight:bold">Swabha Swayamdipta</span> </div> <div class="periodical"> <em>In Proc. of ICML</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/kawine/dataset_difficulty" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> <a class="abstract badge" role="button"><span style="color:var(--global-theme-color);font-weight:bold">Outstanding Paper Award</span></a> </div> <div class="abstract hidden"> <p>Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty ‚Äì w.r.t. a model ùí± ‚Äì as the lack of ùí±-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for ùí±. We further introduce pointwise ÓâÇ-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, ùí±-usable information and PVI also permit the converse: for a given model ùí±, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="pillutla2021mauve" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2102.01454" target="_blank" rel="external nofollow noopener"><b>MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers</b></a></div> <div class="author"> Krishna Pillutla ,¬†<span style="font-weight:bold">Swabha Swayamdipta</span>,¬†Rowan Zellers ,¬†John Thickstun ,¬†Sean Wellecks ,¬†Yejin Choi ,¬†and¬†Zaid Harchaoui </div> <div class="periodical"> <em>In Proc. of NeurIPS</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/krishnap25/mauve" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> <a class="abstract badge" role="button"><span style="color:var(--global-theme-color);font-weight:bold">Outstanding Paper Award</span></a> </div> <div class="abstract hidden"> <p>As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP</abbr> </div> <div id="swayamdipta2020datamaps" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2009.10795" target="_blank" rel="external nofollow noopener"><b>Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics</b></a></div> <div class="author"> <span style="font-weight:bold">Swabha Swayamdipta</span>,¬†Roy Schwartz ,¬†Nicholas Lourie ,¬†Yizhong Wang ,¬†Hannaneh Hajishirzi ,¬†Noah A. Smith ,¬†and¬†Yejin Choi </div> <div class="periodical"> <em>In Proc. of EMNLP</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/allenai/cartography" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> <a href="https://slideslive.com/38939175" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Slides</span></a> </div> <div class="abstract hidden"> <p>Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps‚Äîa model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example‚Äîthe model‚Äôs confidence in the true class, and the variability of this confidence across epochs‚Äîobtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of "ambiguous" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are "easy to learn" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds "hard to learn"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL</abbr> </div> <div id="gururangan2020dont" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2004.10964" target="_blank" rel="external nofollow noopener"><b>Don‚Äôt Stop Pretraining: Adapt Language Models to Domains and Tasks</b></a></div> <div class="author"> Suchin Gururangan ,¬†Ana Marasoviƒá ,¬†<span style="font-weight:bold">Swabha Swayamdipta</span>,¬†Kyle Lo ,¬†Iz Beltagy ,¬†Doug Downey ,¬†and¬†Noah A. Smith </div> <div class="periodical"> <em>In Proc. of ACL</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/allenai/dont-stop-pretraining" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> <a class="abstract badge" role="button"><span style="color:var(--global-theme-color);font-weight:bold">Best Paper Honorable Mention</span></a> </div> <div class="abstract hidden"> <p>Language models pretrained on text from a wide variety of sources form the foundation of today‚Äôs NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task‚Äôs unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NAACL</abbr> </div> <div id="Gururangan:18" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/1803.02324" target="_blank" rel="external nofollow noopener"><b>Annotation Artifacts in Natural Language Inference Data</b></a></div> <div class="author"> Suchin Gururangan ,¬†<span style="font-weight:bold">Swabha Swayamdipta</span>,¬†Omer Levy ,¬†Roy Schwartz ,¬†Samuel Bowman ,¬†and¬†Noah A. Smith </div> <div class="periodical"> <em>In Proc. of NAACL</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/swabhs/notebooks/blob/master/annotation_artifacts.ipynb" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> <a href="/assets/pdf/posters/artifacts-naacl.pdf" class="abstract badge" role="button"><span style="color:cadetblue">Poster</span></a> </div> <div class="abstract hidden"> <p>Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%73%77%61%62%68%61%73@%75%73%63.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-5851-8254" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=3uTVQt0AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2705113" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/swabhs" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://twitter.com/swabhz" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2026 Swabha Swayamdipta. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>