---
layout: page
permalink: /bio/
title: bio
description:
nav: false
nav_order:
---

Swabha Swayamdipta is an Assistant Professor of Computer Science and a co-Associate Director of the Center for AI and Society at the University of Southern California. Her research interests are in natural language processing and machine learning, with a primary interest in the evaluation of generative models of language, understanding the behavior of language models, and designing language technologies for societal good. At USC, Swabha leads the Data, Interpretability, Language and Learning (DILL) Lab.  She received her PhD from Carnegie Mellon University, followed by a postdoc at the Allen Institute for AI and the University of Washington. Her work has received outstanding paper awards at EMNLP 2024, ICML 2022, NeurIPS 2021 and an honorable mention for the best paper at ACL 2020. Her research is supported by awards from NSF, Apple, the Allen Institute for AI, Intel Labs, the Zumberge Foundation and a WiSE Gabilan Fellowship.




<!-- Swabha Swayamdipta is a postdoctoral researcher at the Allen Institute for AI and soon-to-be the Gabilan Assistant Professor and an Assistant Professor of Computer Science at the University of Southern California (starting Fall 2022). Her research interests are in natural language processing, with a focus on studying data distributions to automatically uncover redundancies, annotation and collection artifacts in data, which result in undesirable model biases. Swabha received her PhD from Carnegie Mellon University, and holds a Masters degree from Columbia University. Her work has received an outstanding paper award at NeurIPS 2021 and an honorable mention award for the best paper at ACL 2020. -->

<!-- *Good biases*, such as [structural inductive biases](https://www.aclweb.org/anthology/D18-1412) help language understanding - check out my [PhD thesis](/assets/pdf/swabha_thesis.pdf) on these. -->
<!-- But biases can be *undesirable*, e.g. [spurious correlations](https://arxiv.org/abs/2002.04108) commonly found in crowd-sourced, large-scale datasets due to [annotation artifacts](https://arxiv.org/abs/1803.02324), or social prejudices of human annotators and task designers, which are [difficult to rid](https://arxiv.org/abs/2102.00086)! -->
